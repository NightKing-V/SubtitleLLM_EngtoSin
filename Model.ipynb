{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e90a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBart50TokenizerFast\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f86704",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_json = \"Subtitle_Dataset/aligned_subtitles.json\"\n",
    "tokenized_file = \"Subtitle_Dataset/tokenized_subtitles.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6527f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa1ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set language codes\n",
    "SRC_LANG = \"en_XX\"\n",
    "TGT_LANG = \"si_LK\"\n",
    "tokenizer.src_lang = SRC_LANG\n",
    "tokenizer.tgt_lang = TGT_LANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8993750b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete. Tensors saved to 'tokenized_subtitles.pt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your cleaned JSON data\n",
    "with open(text_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare inputs\n",
    "sources = [item[\"en\"] for item in data]\n",
    "targets = [item[\"si\"] for item in data]\n",
    "\n",
    "# ‚úÖ New way: tokenize source + target in one call\n",
    "tokenized_data = tokenizer(\n",
    "    sources,\n",
    "    text_target=targets,\n",
    "    max_length=128,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "torch.save(tokenized_data, tokenized_file)\n",
    "\n",
    "print(\"‚úÖ Tokenization complete. Tensors saved to 'tokenized_subtitles.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a908f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "with open(text_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Optional: Split train/val\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69774d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53906893b0364b1da68092c547282afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4059 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729f51d1716841e397c309df7668b480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"si_LK\"\n",
    "\n",
    "def preprocess(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"en\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        example[\"si\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b0468d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_874/4207007079.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1524' max='1524' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1524/1524 2:59:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.839800</td>\n",
       "      <td>0.992283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.969998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.835300</td>\n",
       "      <td>0.970196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1524, training_loss=1.191796876001233, metrics={'train_runtime': 10783.9569, 'train_samples_per_second': 1.129, 'train_steps_per_second': 0.141, 'total_flos': 3298642398019584.0, 'train_loss': 1.191796876001233, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints-mbart50-en-si\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True  # if using GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# üèÅ Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0cb003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mbart50-en-si/tokenizer_config.json',\n",
       " 'mbart50-en-si/special_tokens_map.json',\n",
       " 'mbart50-en-si/sentencepiece.bpe.model',\n",
       " 'mbart50-en-si/added_tokens.json',\n",
       " 'mbart50-en-si/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"mbart50-en-si\")\n",
    "tokenizer.save_pretrained(\"mbart50-en-si\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d297fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"mbart50-en-si\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"mbart50-en-si\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example format\n",
    "# [{\"en\": \"...\", \"si\": \"...\"}, ...]\n",
    "import random\n",
    "\n",
    "with open(text_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "eval_sample = random.sample(eval_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf7fcd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [07:50<00:00,  9.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for example in tqdm(eval_sample):\n",
    "    input_text = example[\"en\"]\n",
    "    reference = example[\"si\"]\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    \n",
    "    # Generate translation\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"si_LK\"],  # Sinhala\n",
    "        max_length=128,\n",
    "        num_beams=5\n",
    "    )\n",
    "\n",
    "    translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    references.append([reference.split()])  # List of references\n",
    "    predictions.append(translated_text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fac33140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ BLEU score: 2.24\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "bleu_score = corpus_bleu(references, predictions)\n",
    "print(f\"üîµ BLEU score: {bleu_score * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd56135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Lord Stark!\n",
      "Translated: ‡∑É‡∑ä‡∂ß‡∑è‡∂ö‡∑ä ‡∂ã‡∂≠‡∑î‡∂∏‡∑è‡∂´‡∑ô‡∂±‡∑í!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for example in tqdm(eval_sample):\n",
    "input_text = \"Lord Stark!\"\n",
    "# reference = example[\"si\"]\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "# Generate translation\n",
    "generated_tokens = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"si_LK\"],  # Sinhala\n",
    "    max_length=128,\n",
    "    num_beams=5\n",
    ")\n",
    "\n",
    "translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# references.append([reference.split()])  # List of references\n",
    "# predictions.append(translated_text.split())\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Translated: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ab607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"mbart50-en-si\",  # your fine-tuned model dir\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mbart50-en-si\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc61dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46.1\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
